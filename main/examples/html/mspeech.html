<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>Micro speech example</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="extra_stylesheet.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="printComponentTabs.js"></script>
<script type="text/javascript" src="footer.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 55px;">
  <td style="padding: 1em;"id="projectlogo"><img alt="Logo" src="arm.png"/></td>
  <td style="padding-left: 2em; padding-bottom: 1em;padding-top: 1em;">
   <div id="projectname">Orta
   &#160;<span id="projectnumber">Version 0.2</span>
   </div>
   <div id="projectbrief">Examples Projects and GitHub Repositories</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<div id="Compnav" class="tabs1">
    <ul class="tablist">
      <script type="text/javascript">
		<!--
		writeComponentTabs.call(this);
		//-->
      </script>
	  </ul>
</div>
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Usage&#160;and&#160;Description</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.svg"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('mspeech.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Micro speech example </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_src_mspeech"></a> An implementation of the micro speech example with VHT support is available at <a href="https://github.com/MDK-Packs/VHT-TFLmicrospeech">github.com/MDK-Packs/VHT-TFLmicrospeech</a>.</p>
<p>The program analyzes an audio input with a voice recognition model that can detect 2 keywords - <b>yes</b> and <b>no</b>. The recognized keywords are then printed into a serial interface. The voice recognition model is implemented using <a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite for Microcontrollers</a>.</p>
<p>The example project can be executed on <a class="el" href="mspeech.html#mspeech_vht">\prj_name</a> as well as on <a class="el" href="mspeech.html#mspeech_hw">hardware</a>. This demonstrates how to use the processor and peripheral abstraction layers for simpler software portability across different targets.</p>
<h1><a class="anchor" id="autotoc_md0"></a>
Structure</h1>
<p>The table below explains the content in the micro speech example's repository.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Folder   </th><th class="markdownTableHeadLeft">Description    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><code>./micro_speech/</code>   </td><td class="markdownTableBodyLeft">Contains the voice recognition model that is used by all targets. This part is similar to the original TF-Lite for Microcontrollers example, with just minor modifications.<br  />
TensorFlow calculation kernel is provided separately via corresponding software packs listed in <a class="el" href="mspeech.html#mspeech_pre">Prerequisites</a> and as explained in <a class="el" href="mspeech.html#tf_variants">TensorFlow-Lite kernel variants</a>.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><code>./Platform_FVP_Corstone_SSE-300_Ethos-U55/</code>   </td><td class="markdownTableBodyLeft">Project files specific for executing the program on VHT. See <a class="el" href="mspeech.html#mspeech_vht">Running on \prj_name</a> for details and execution instructions.    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><code>./Platform_MIMXRT1064-EVK/</code>   </td><td class="markdownTableBodyLeft">Project files specific for executing the program on MIMXRT1064-EVK. See <a class="el" href="mspeech.html#mspeech_hw">Running on a hardware target</a> for details and execution instructions.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><code>./VSI</code>   </td><td class="markdownTableBodyLeft">Implementation of Audio Streaming Interface via VSI. See <a href="../../simulation/html/index.html"><b>Simulation</b></a> chapter for details.   </td></tr>
</table>
<h1><a class="anchor" id="mspeech_pre"></a>
Prerequisites</h1>
<p>Following items are required for running the micro speech example on a PC:</p>
<ul>
<li><b>Toolchain</b><ul>
<li>IDE (Windows only): <a href="https://www.keil.com/mdk5">MDK Microcontroller Development Kit</a></li>
<li>alternatively, command-line building tool: <a href="https://github.com/ARM-software/CMSIS_5/releases/download/5.7.0/cbuild_install.0.10.2.sh">CMSIS-Build</a></li>
</ul>
</li>
<li><a href="https://developer.arm.com/tools-and-software/embedded/cmsis/cmsis-packs"><b>Common public CMSIS Software Packs</b></a><ul>
<li>ARM::CMSIS 5.8.0</li>
<li>Keil::ARM_Compiler 1.6.3</li>
</ul>
</li>
<li><b>Non-public TensorFlow CMSIS Software Packs</b><ul>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/tensorflow.flatbuffers.0.1.20210719.pack">tensorflow.flatbuffers.0.1.20210719</a></li>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/tensorflow.gemmlowp.0.0.1.20210719.pack">tensorflow.gemmlowp.0.0.1.20210719</a></li>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/tensorflow.kissfft.0.1.20210719.pack">kissfft.0.1.20210719</a></li>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/tensorflow.ruy.0.1.20210719.pack">tensorflow.ruy.0.1.20210719</a></li>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/tensorflow.tensorflow-lite-micro.0.2.20210719.pack">tensorflow.tensorflow-lite-micro.0.2.20210719</a></li>
<li><a href="https://github.com/MDK-Packs/tensorflow-pack/releases/download/preview-0.3/Arm.ethos-u-core-driver.0.1.20210719.pack">Arm.ethos-u-core-driver.0.1.20210719</a> (for virtual target with Ethos-U55)</li>
</ul>
</li>
<li>**Specific for \prj_name**<ul>
<li><a href="https://developer.arm.com/tools-and-software/embedded/cmsis/cmsis-packs">public CMSIS software packs</a> for target support:<ul>
<li>ARM::V2M_MPS3_SSE_300_BSP 1.2.0</li>
</ul>
</li>
<li>for targets with VSI support<ul>
<li><a href="https://www.python.org/downloads/">Python 3.9</a></li>
<li><a href="https://developer.arm.com/tools-and-software/simulation-models/fast-models">Fast Models</a> 11.15</li>
<li>FVP model for Corstone-300 MPS3 with VSI support<ul>
<li>The prebuilt model binaries are available as assets in Git under <a href="https://github.com/RobertRostohar/Orta/releases"><b>Releases</b></a>. The contents of <code>FVP_VSI_Corstone_SSE-300_Ethos-U55_xxx.zip</code> shall be extracted into directory <code>./VHT/Build_Corstone_SSE-300_Ethos-U55/system</code>.</li>
<li>Alternatively, the model executable can be built manually as explained in <code>./VHT/README.md</code>.</li>
</ul>
</li>
</ul>
</li>
<li>for targets without VSI support<ul>
<li><a href="https://developer.arm.com/tools-and-software/open-source-software/arm-platforms-software/arm-ecosystem-fvps">Ecosystem FVP for Corstone-300 MPS3</a></li>
</ul>
</li>
</ul>
</li>
<li><b>Specific for HW target</b><ul>
<li><a href="https://www.nxp.com/design/development-boards/i-mx-evaluation-and-development-boards/mimxrt1064-evk-i-mx-rt1064-evaluation-kit:MIMXRT1064-EVK">NXP MIMXRT1064-EVK</a></li>
<li><a href="https://developer.arm.com/tools-and-software/embedded/cmsis/cmsis-packs">public CMSIS packs</a>:<ul>
<li>ARM::CMSIS-Driver 2.6.1</li>
<li>NXP::MIMXRT1064_DFP 13.0.0</li>
<li>NXP::EVK-MIMXRT1064_BSP 13.0.0</li>
<li>Keil::MIMXRT1064-EVK_BSP 1.2.1</li>
<li>Keil::iMXRT1064_MWP 1.4.0</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md1"></a>
Program Execution</h1>
<p>The example project can be executed on <a class="el" href="mspeech.html#mspeech_vht">\prj_name</a> as well as on <a class="el" href="mspeech.html#mspeech_hw">hardware</a> as explained in this section.</p>
<h2><a class="anchor" id="mspeech_vht"></a>
Running on \prj_name</h2>
<p>Directory <code>./VHT-TFLmicrospeech/Platform_FVP_Corstone_SSE-300_Ethos-U55/</code> in the example repository contains the project files for executing the program on VHT.</p>
<p>The project can be executed on two types of targets:</p><ul>
<li><b>with VSI support</b>: implements audio driver on VSI peripheral with audio test data provided through Python script as explained in <a href="../../simulation/html/index.html"><b>Simulation</b></a> chapter.</li>
<li><b>without VSI support</b>: integrates audio test data into embedded code and can be run on ecosystem FVPs without VSI support.</li>
</ul>
<p>Below is the description of available project targets with execution instructions:</p>
<p><b>Example</b> target runs on a VHT <b>with VSI support</b>:</p><ul>
<li>Uses customized Corstone SSE-300 Ethos-U55 FVP with Virtual Streaming Interface (VSI). Refer to virtual platform setup described in <a class="el" href="mspeech.html#mspeech_pre">Prerequisites</a>.</li>
<li>Audio test data is provided by the Python script <code>./VHT-TFLmicrospeech/VSI/audio/python/arm_vsi0.py</code> from the WAVE file <code>./VHT-TFLmicrospeech/Platform_FVP_Corstone_SSE-300_Ethos-U55/test.wav</code> which contains keywords 'Yes' and 'No' alternating three times.</li>
<li>Open the example with Keil MDK (Windows only) using the uVision project <code>./VHT-TFLmicrospeech/Platform_FVP_Corstone_SSE-300_Ethos-U55/microspeech.uvprojx</code> and build it for target <code>Example</code>.</li>
<li>Alternatively compile with CMSIS-Build using <code>microspeech.Example.cprj</code> project.</li>
<li>Run the compiled example with MDK or standalone with script <code>run_example.cmd</code>.</li>
<li>When running the example the audio data is processed and detected keywords are output to the Telnet terminal with their time stamps in the audio stream. Following output can be observed for the default test.wav file included with the example: <br  />
<code> Heard yes (152) @1100ms <br  />
 Heard no (141) @5500ms <br  />
 Heard yes (147) @9100ms <br  />
 Heard no (148) @13600ms <br  />
 Heard yes (147) @17100ms <br  />
 Heard no (148) @21600ms <br  />
 </code></li>
</ul>
<p><b>Example Test</b> is an internal test for the Example target<br  />
</p>
<p><b>Audio Provider Mock</b> runs on a VHT <b>without VSI support</b>:</p><ul>
<li>Uses <a href="https://developer.arm.com/tools-and-software/open-source-software/arm-platforms-software/arm-ecosystem-fvps"><b>ecosystem FVP for Corstone-300 MPS3</b></a>.<ul>
<li>By default following path is used for the FVP: <code>c:\Program Files\ARM\FVP_Corstone_SSE-300_Ethos-U55\models\Win64_VC2017\FVP_Corstone_SSE-300_Ethos-U55.exe</code>. Update it in the uVision project or in the execution script if necessary.</li>
</ul>
</li>
<li>Audio test data is embedded in the test code and contains keywords 'Yes' and 'No' alternating indefinitely.</li>
<li>Build example with MDK using uVision project <code>microspeech.uvprojx</code> target <code>Audio Provider Mock</code> or with CMSIS-Build using <code>microspeech.Audio_Provider_Mock.cprj</code> project.</li>
<li>Run example with MDK or standalone with script <code>run_audio_provider_mock.cmd</code>.</li>
<li>When running the example the audio data is processed and detected keywords are continuously output to the Telnet terminal with their time stamps in the audio stream. Following output can be observed for the default test.wav file included with the example: <br  />
<code> Heard silence (149) @400ms <br  />
 Heard yes (158) @1200ms <br  />
 Heard no (143) @5600ms <br  />
 Heard yes (149) @9100ms <br  />
 Heard no (142) @13600ms <br  />
 Heard yes (149) @17100ms <br  />
 Heard no (142) @21600ms <br  />
 ... </code></li>
</ul>
<p><b>Audio Provider Mock Test</b>: internal test for Audio Provider Mock target</p>
<h2><a class="anchor" id="mspeech_hw"></a>
Running on a hardware target</h2>
<p>Directory <code>./VHT-TFLmicrospeech/Platform_MIMXRT1064-EVK/</code> in the example repository contains the project files for executing the program on <a href="https://www.nxp.com/design/development-boards/i-mx-evaluation-and-development-boards/mimxrt1064-evk-i-mx-rt1064-evaluation-kit:MIMXRT1064-EVK">NXP MIMXRT1064-EVK</a> development board with an Arm Cortex-M7 processor. One target <b>MIMXRT1064-EVK</b> is provided in the project.</p>
<p>This project uses the on-board microphone for audio input and prints recognized keywords to the serial interface.</p>
<p>The hardware setup is simple and requires only connection to the PC via USB. The project is configured to load the program to QSPI NOR flash so the boot switch SW7 shall be set to 0010.</p>
<p><img src="mimxrt1064_hw_setup.png" alt="" class="inline" title="MIMXRT1064-EVK board"/></p>
<p>Execute the program in following steps:</p><ul>
<li>Build example with MDK using uVision project <code>microspeech.uvprojx</code> or with CMSIS-Build using <code>microspeech.MIMXRT1064-EVK.cprj</code> project.</li>
<li>Program and run the example with MDK or use Drag-and-drop programming through the DAP-Link drive.</li>
<li>Open the DAP-Link Virtual COM port in a terminal (baudrate = 115200), speak the keywords and monitor recognition results in the terminal window.</li>
</ul>
<h1><a class="anchor" id="tf_variants"></a>
TensorFlow-Lite kernel variants</h1>
<p>The micro speech example uses <b>tensorflow-lite-micro</b> pack that contains <b>Machine Learning</b> software component implementing among others the universal kernel for executing TensorFlow ML operations independent from the actual load type (audio, video, or others).</p>
<p>Implementation of these kernel operations is available in several variants optimized for Arm targets. When using the uVision project the variant can be selected in <b>Manage Run-Time Environment</b> window as shown on the picture below.</p>
<p><img src="mdk_tf_kernel_variants.png" alt="" class="inline" title="TF-Lite Kernel component variants"/></p>
<p>When using CMSIS-Build the kernel variant is specified in the .cprj project file in the line: </p><div class="fragment"><div class="line">&lt;component Cclass=&quot;Machine Learning&quot; Cgroup=&quot;TensorFlow&quot; Csub=&quot;Kernel&quot; Cvariant=&quot;CMSIS-NN&quot; Cvendor=&quot;tensorflow&quot;/&gt;</div>
</div><!-- fragment --><p> Following kernel variants are available:</p><ul>
<li><b>CMSIS-NN</b><br  />
Optimizes execution on Cortex-M devices by using mathematical functions from <a href="https://arm-software.github.io/CMSIS_5/NN/html/index.html">CMSIS-NN software library</a>.<br  />
 Underlying implementations automatically utilize target-specific hardware extensions such as <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/helium"><b>Helium</b></a> (or MVE: M-Profile Vector Extensions) and <a href="https://developer.arm.com/architectures/instruction-sets/dsp-extensions/dsp-for-cortex-m"><b>SIMD</b></a> (Single Instruction Multiple Data) and so maximize computing performance and reduce code footprint.<br  />
 For devices with MVE (such as Cortex-M55), there is additional configuration field <b>Vector Extensions</b> in the <b>Options for Target..</b>-<a href="https://www.keil.com/support/man/docs/uv4/uv4_dg_adstarg.htm"><b>Target</b></a> dialog, that specifies MVE use in the project. <img src="mdk_conf_mve.png" alt="" class="inline" title="MVE configuration in uVision"/></li>
</ul>
<ul>
<li><b>Ethos-U</b><br  />
(Not functional at this moment). Uses implementation optimized for Arm Ethos-U NPUs.</li>
<li><b>Reference</b><br  />
Target-independent software implementation is used. Fallback solution for operations that cannot be optimized for target hardware.</li>
</ul>
<h1><a class="anchor" id="autotoc_md2"></a>
Performance measurement</h1>
<p>File <code>./micro_speech/src/main_functions.cc</code> in the repository example contains <a href="https://www.keil.com/pack/doc/compiler/EventRecorder/html/es_use.html">Event Statistics</a> annotations that allow to measure performance of ML algorithms in different configurations. This currently works only in setup with Keil MDK. <a href="https://armkeil.blob.core.windows.net/developer/Files/videos/KeilMDK/yDBbd3lQehXV.mp4">This video</a> demonstrates program execution including the views into Event Statistics.</p>
<p>There are three events defined:</p><ul>
<li><b>Event C0</b>: signal processing part that creates a spectrogram from the current data slice.</li>
<li><b>Event C1</b>: ML inference algorithm for the created spectrogram. Most compute-intensive part.</li>
<li><b>Event C2</b>: Verifies if a keyword was detected.</li>
</ul>
<p>Execute the program with different <a class="el" href="mspeech.html#tf_variants">TensorFlow-Lite kernel variants</a> and compare the Event Statistics output. Implementations optimized for Arm hardware extensions provide significant performance improvement C1 measurement and also shorten C0 duration. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">Examples</a></li>
    <li class="footer">
	Copyright &copy; 2021 Arm Limited (or its affiliates). All rights reserved.
	<!-- Use script below for auto-inserting date, project name, version, etc  -->
	<!-- <script type="text/javascript">
        writeFooter.call(this);
    </script> -->
    </li>
  </ul>
</div>
</body>
</html>
